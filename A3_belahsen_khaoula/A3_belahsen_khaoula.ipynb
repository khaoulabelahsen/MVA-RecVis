{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_belahsen_khaoula.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF93lyuy8XvY",
        "colab_type": "code",
        "outputId": "1bf88ad1-4a9b-40ee-c44c-fa4b0d6a7623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Adapted from : https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNPaEQvq8k67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gosaCedevbOB",
        "colab_type": "code",
        "outputId": "050206c3-93a4-46ec-9abb-1d3d0fd64e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      if i<len(self.datasets[0]):\n",
        "        return self.datasets[0][i]\n",
        "      else :\n",
        "        return self.datasets[1][len(self.datasets[0])-i]\n",
        "        #return tuple(d[i] for d in self.datasets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(d) for d in self.datasets)\n",
        "data1 = ConcatDataset(datasets.ImageFolder(args[\"data\"] + '/train_images',\n",
        "                         transform=data_transforms_train),\n",
        "                 datasets.ImageFolder(args[\"data\"] + '/val_images',\n",
        "                        transform=data_transforms_val))\n",
        "validation_split=0.3\n",
        "random_seed= 42\n",
        "shuffle_dataset=True\n",
        "dataset_size = len(data1)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader1 = torch.utils.data.DataLoader(data1, batch_size=args[\"batch_size\"], \n",
        "                                           sampler=train_sampler)\n",
        "val_loader1 = torch.utils.data.DataLoader(data1, batch_size=args[\"batch_size\"],\n",
        "                                                sampler=valid_sampler)\n",
        "\n",
        "set_loader = torch.utils.data.DataLoader(\n",
        "             ConcatDataset(\n",
        "                 datasets.ImageFolder(args[\"data\"] + '/train_images',\n",
        "                         transform=data_transforms_train),\n",
        "                 datasets.ImageFolder(args[\"data\"] + '/val_images',\n",
        "                         transform=data_transforms_val)),\n",
        "             batch_size=args[\"batch_size\"], shuffle=True,\n",
        "             num_workers=1)\n",
        "#, pin_memory=True)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass ConcatDataset(torch.utils.data.Dataset):\\n    def __init__(self, *datasets):\\n        self.datasets = datasets\\n\\n    def __getitem__(self, i):\\n      if i<len(self.datasets[0]):\\n        return self.datasets[0][i]\\n      else :\\n        return self.datasets[1][len(self.datasets[0])-i]\\n        #return tuple(d[i] for d in self.datasets)\\n\\n    def __len__(self):\\n        return sum(len(d) for d in self.datasets)\\ndata1 = ConcatDataset(datasets.ImageFolder(args[\"data\"] + \\'/train_images\\',\\n                         transform=data_transforms_train),\\n                 datasets.ImageFolder(args[\"data\"] + \\'/val_images\\',\\n                        transform=data_transforms_val))\\nvalidation_split=0.3\\nrandom_seed= 42\\nshuffle_dataset=True\\ndataset_size = len(data1)\\nindices = list(range(dataset_size))\\nsplit = int(np.floor(validation_split * dataset_size))\\nif shuffle_dataset :\\n    np.random.seed(random_seed)\\n    np.random.shuffle(indices)\\ntrain_indices, val_indices = indices[split:], indices[:split]\\n\\n# Creating PT data samplers and loaders:\\ntrain_sampler = SubsetRandomSampler(train_indices)\\nvalid_sampler = SubsetRandomSampler(val_indices)\\n\\ntrain_loader1 = torch.utils.data.DataLoader(data1, batch_size=args[\"batch_size\"], \\n                                           sampler=train_sampler)\\nval_loader1 = torch.utils.data.DataLoader(data1, batch_size=args[\"batch_size\"],\\n                                                sampler=valid_sampler)\\n\\nset_loader = torch.utils.data.DataLoader(\\n             ConcatDataset(\\n                 datasets.ImageFolder(args[\"data\"] + \\'/train_images\\',\\n                         transform=data_transforms_train),\\n                 datasets.ImageFolder(args[\"data\"] + \\'/val_images\\',\\n                         transform=data_transforms_val)),\\n             batch_size=args[\"batch_size\"], shuffle=True,\\n             num_workers=1)\\n#, pin_memory=True)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSouJ3MO8dHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# once the images are loaded, how do we pre-process them before being passed into the network\n",
        "# by default, we resize the images to 64 x 64 in size\n",
        "# and normalize them to mean = 0 and standard-deviation = 1 based on statistics collected from\n",
        "# the training set\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "data_transforms_train = transforms.Compose([\n",
        "        #transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "data_transforms_val = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvghB6Vg88iT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Importing the datasets \n",
        "\n",
        "dataset = '/content/drive/My Drive/recvis19_a3-master/bird_dataset'\n",
        "batch_size = 20\n",
        "\n",
        "train_datasets = datasets.ImageFolder(dataset + '/train_images',\n",
        "                         transform=data_transforms_train)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "val_datasets =  datasets.ImageFolder(dataset + '/val_images',transform=data_transforms_val)\n",
        "val_loader = torch.utils.data.DataLoader(val_datasets,batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "\n",
        "dataloaders = {'train':train_loader, 'val':val_loader}\n",
        "dataset_sizes = {'train':len(train_datasets), 'val':len(val_datasets)}\n",
        "\n",
        "class_names_train = train_datasets.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AULGJxz1AacU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "\n",
        "  model.train()\n",
        "  correct = 0\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    if use_cuda:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "      if batch_idx % log_interval == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.data.item()), 'Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        correct, len(train_loader.dataset),\n",
        "        100. * correct / len(train_loader.dataset))) \n",
        "\n",
        "def validation():\n",
        "\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "        validation_loss += criterion(output, target).data.item()\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV22nuQhAaaj",
        "colab_type": "code",
        "outputId": "f23381c5-b20c-4032-aea8-4100e88411d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "import easydict\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "nclasses = 20\n",
        "data = '/content/gdrive/My Drive/recvis/bird_dataset'\n",
        "batch_size =20\n",
        "epoch =  20\n",
        "lr =  0.1\n",
        "momentum = 0.9\n",
        "seed = 1\n",
        "log_interval = 10\n",
        "experiment = '/content/gdrive/My Drive/recvis/experiment'\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Create experiment folder\n",
        "if not os.path.isdir(experiment):\n",
        "    os.makedirs(experiment)\n",
        "\n",
        "# Data initialization and loading\n",
        "#from data import data_transforms\n",
        "\n",
        "# Neural network and optimizer\n",
        "# We define neural net in model.py so that it can be reused by the evaluate.py script\n",
        "\n",
        "def model_choice(name='resnet152'):\n",
        "  if name =='vgg19_bn':\n",
        "    model_ft = models.vgg19_bn(pretrained='imagenet')\n",
        "    model_conv = nn.Sequential(*list(model.children())[:-1])\n",
        "    for param in model_conv.parameters():\n",
        "        param.requires_grad = False\n",
        "    model_ft.classifier = nn.Sequential(\n",
        "        nn.Linear(512 * 7 * 7, 4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, nclasses),\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    if name=='resnet152':\n",
        "      model_ft = models.resnet152(pretrained=True)\n",
        "      num_ftrs = model_ft.fc.in_features\n",
        "      model_ft.fc = nn.Linear(num_ftrs, 20)\n",
        "    if name=='resnet101':\n",
        "      model_ft = models.resnet101(pretrained=True)\n",
        "      num_ftrs = model_ft.fc.in_features\n",
        "      model_ft.fc = nn.Linear(num_ftrs, 20)\n",
        "    elif name=='resnet18':\n",
        "      model_ft = models.resnet18(pretrained=True)\n",
        "      num_ftrs = model_ft.fc.in_features\n",
        "      model_ft.fc = nn.Linear(num_ftrs, 20)\n",
        "    else:\n",
        "      model_ft = models.resnet50(pretrained=True)\n",
        "      num_ftrs = model_ft.fc.in_features\n",
        "      model_ft.fc = nn.Linear(num_ftrs, 20)\n",
        "      \n",
        "  return model_ft.to(device)\n",
        "\n",
        "name = 'resnet101'\n",
        "model = model_choice(name=name)\n",
        "'''\n",
        "for name, child in model.named_children():\n",
        "   if name in ['fc','layer4','layer3']:\n",
        "       print(name + ' is unfrozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = True\n",
        "   else:\n",
        "       print(name + ' is frozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = False\n",
        "'''\n",
        "if use_cuda:\n",
        "    print('Using GPU')\n",
        "    model.cuda()\n",
        "else:\n",
        "    print('Using CPU')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "#best_acc = 0\n",
        "for epoch in range(1, epoch + 1):\n",
        "    train(epoch)\n",
        "    validation()\n",
        "    #acc = 100. * correct / len(val_loader.dataset)\n",
        "    #if acc > best_acc : \n",
        "      #best_acc = acc\n",
        "      #best_model = copy.deepcopy(model.state_dict())     \n",
        "    model_file = experiment + '/model_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('Saved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')\n",
        "\n",
        "#model.load_state_dict(best_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n",
            "100%|██████████| 170M/170M [00:02<00:00, 66.3MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Train Epoch: 1 [0/1090 (0%)]\tLoss: 3.136143 Accuracy: 0/1090 (0%)\n",
            "Train Epoch: 1 [200/1090 (18%)]\tLoss: 3.067652 Accuracy: 10/1090 (1%)\n",
            "Train Epoch: 1 [400/1090 (36%)]\tLoss: 2.743560 Accuracy: 32/1090 (3%)\n",
            "Train Epoch: 1 [600/1090 (55%)]\tLoss: 2.642950 Accuracy: 85/1090 (8%)\n",
            "Train Epoch: 1 [800/1090 (73%)]\tLoss: 2.373545 Accuracy: 163/1090 (15%)\n",
            "Train Epoch: 1 [1000/1090 (91%)]\tLoss: 2.001949 Accuracy: 253/1090 (23%)\n",
            "\n",
            "Validation set: Average loss: 0.0972, Accuracy: 66/103 (64%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_1.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_1.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 2 [0/1090 (0%)]\tLoss: 2.049935 Accuracy: 12/1090 (1%)\n",
            "Train Epoch: 2 [200/1090 (18%)]\tLoss: 1.693794 Accuracy: 137/1090 (13%)\n",
            "Train Epoch: 2 [400/1090 (36%)]\tLoss: 1.409563 Accuracy: 271/1090 (25%)\n",
            "Train Epoch: 2 [600/1090 (55%)]\tLoss: 1.597973 Accuracy: 409/1090 (38%)\n",
            "Train Epoch: 2 [800/1090 (73%)]\tLoss: 1.108951 Accuracy: 540/1090 (50%)\n",
            "Train Epoch: 2 [1000/1090 (91%)]\tLoss: 1.138583 Accuracy: 683/1090 (63%)\n",
            "\n",
            "Validation set: Average loss: 0.0422, Accuracy: 85/103 (83%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_2.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_2.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 3 [0/1090 (0%)]\tLoss: 1.010629 Accuracy: 17/1090 (2%)\n",
            "Train Epoch: 3 [200/1090 (18%)]\tLoss: 1.134971 Accuracy: 163/1090 (15%)\n",
            "Train Epoch: 3 [400/1090 (36%)]\tLoss: 0.793598 Accuracy: 323/1090 (30%)\n",
            "Train Epoch: 3 [600/1090 (55%)]\tLoss: 0.874034 Accuracy: 470/1090 (43%)\n",
            "Train Epoch: 3 [800/1090 (73%)]\tLoss: 1.262943 Accuracy: 602/1090 (55%)\n",
            "Train Epoch: 3 [1000/1090 (91%)]\tLoss: 0.889657 Accuracy: 750/1090 (69%)\n",
            "\n",
            "Validation set: Average loss: 0.0269, Accuracy: 88/103 (85%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_3.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_3.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 4 [0/1090 (0%)]\tLoss: 0.651886 Accuracy: 16/1090 (1%)\n",
            "Train Epoch: 4 [200/1090 (18%)]\tLoss: 0.703864 Accuracy: 170/1090 (16%)\n",
            "Train Epoch: 4 [400/1090 (36%)]\tLoss: 0.774096 Accuracy: 327/1090 (30%)\n",
            "Train Epoch: 4 [600/1090 (55%)]\tLoss: 0.539473 Accuracy: 482/1090 (44%)\n",
            "Train Epoch: 4 [800/1090 (73%)]\tLoss: 0.601413 Accuracy: 647/1090 (59%)\n",
            "Train Epoch: 4 [1000/1090 (91%)]\tLoss: 0.408041 Accuracy: 801/1090 (73%)\n",
            "\n",
            "Validation set: Average loss: 0.0254, Accuracy: 90/103 (87%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_4.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_4.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 5 [0/1090 (0%)]\tLoss: 0.709326 Accuracy: 15/1090 (1%)\n",
            "Train Epoch: 5 [200/1090 (18%)]\tLoss: 0.423123 Accuracy: 186/1090 (17%)\n",
            "Train Epoch: 5 [400/1090 (36%)]\tLoss: 0.268047 Accuracy: 350/1090 (32%)\n",
            "Train Epoch: 5 [600/1090 (55%)]\tLoss: 0.960798 Accuracy: 508/1090 (47%)\n",
            "Train Epoch: 5 [800/1090 (73%)]\tLoss: 0.639686 Accuracy: 667/1090 (61%)\n",
            "Train Epoch: 5 [1000/1090 (91%)]\tLoss: 0.268208 Accuracy: 824/1090 (76%)\n",
            "\n",
            "Validation set: Average loss: 0.0194, Accuracy: 88/103 (85%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_5.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_5.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 6 [0/1090 (0%)]\tLoss: 0.444138 Accuracy: 18/1090 (2%)\n",
            "Train Epoch: 6 [200/1090 (18%)]\tLoss: 0.894450 Accuracy: 185/1090 (17%)\n",
            "Train Epoch: 6 [400/1090 (36%)]\tLoss: 0.739807 Accuracy: 343/1090 (31%)\n",
            "Train Epoch: 6 [600/1090 (55%)]\tLoss: 0.395134 Accuracy: 519/1090 (48%)\n",
            "Train Epoch: 6 [800/1090 (73%)]\tLoss: 0.992599 Accuracy: 687/1090 (63%)\n",
            "Train Epoch: 6 [1000/1090 (91%)]\tLoss: 0.545619 Accuracy: 851/1090 (78%)\n",
            "\n",
            "Validation set: Average loss: 0.0183, Accuracy: 92/103 (89%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_6.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_6.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 7 [0/1090 (0%)]\tLoss: 0.611499 Accuracy: 18/1090 (2%)\n",
            "Train Epoch: 7 [200/1090 (18%)]\tLoss: 0.781626 Accuracy: 198/1090 (18%)\n",
            "Train Epoch: 7 [400/1090 (36%)]\tLoss: 0.654610 Accuracy: 378/1090 (35%)\n",
            "Train Epoch: 7 [600/1090 (55%)]\tLoss: 0.555332 Accuracy: 542/1090 (50%)\n",
            "Train Epoch: 7 [800/1090 (73%)]\tLoss: 0.510094 Accuracy: 717/1090 (66%)\n",
            "Train Epoch: 7 [1000/1090 (91%)]\tLoss: 0.457248 Accuracy: 887/1090 (81%)\n",
            "\n",
            "Validation set: Average loss: 0.0161, Accuracy: 92/103 (89%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_7.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_7.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 8 [0/1090 (0%)]\tLoss: 0.296793 Accuracy: 19/1090 (2%)\n",
            "Train Epoch: 8 [200/1090 (18%)]\tLoss: 0.309627 Accuracy: 187/1090 (17%)\n",
            "Train Epoch: 8 [400/1090 (36%)]\tLoss: 0.411286 Accuracy: 358/1090 (33%)\n",
            "Train Epoch: 8 [600/1090 (55%)]\tLoss: 0.551584 Accuracy: 526/1090 (48%)\n",
            "Train Epoch: 8 [800/1090 (73%)]\tLoss: 0.271231 Accuracy: 696/1090 (64%)\n",
            "Train Epoch: 8 [1000/1090 (91%)]\tLoss: 0.388009 Accuracy: 871/1090 (80%)\n",
            "\n",
            "Validation set: Average loss: 0.0159, Accuracy: 93/103 (90%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_8.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_8.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 9 [0/1090 (0%)]\tLoss: 0.289683 Accuracy: 19/1090 (2%)\n",
            "Train Epoch: 9 [200/1090 (18%)]\tLoss: 0.277614 Accuracy: 195/1090 (18%)\n",
            "Train Epoch: 9 [400/1090 (36%)]\tLoss: 0.328573 Accuracy: 373/1090 (34%)\n",
            "Train Epoch: 9 [600/1090 (55%)]\tLoss: 0.250624 Accuracy: 547/1090 (50%)\n",
            "Train Epoch: 9 [800/1090 (73%)]\tLoss: 0.214348 Accuracy: 719/1090 (66%)\n",
            "Train Epoch: 9 [1000/1090 (91%)]\tLoss: 0.554551 Accuracy: 891/1090 (82%)\n",
            "\n",
            "Validation set: Average loss: 0.0165, Accuracy: 89/103 (86%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_9.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_9.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 10 [0/1090 (0%)]\tLoss: 0.368140 Accuracy: 17/1090 (2%)\n",
            "Train Epoch: 10 [200/1090 (18%)]\tLoss: 0.467499 Accuracy: 196/1090 (18%)\n",
            "Train Epoch: 10 [400/1090 (36%)]\tLoss: 0.484761 Accuracy: 362/1090 (33%)\n",
            "Train Epoch: 10 [600/1090 (55%)]\tLoss: 0.346062 Accuracy: 540/1090 (50%)\n",
            "Train Epoch: 10 [800/1090 (73%)]\tLoss: 0.385446 Accuracy: 718/1090 (66%)\n",
            "Train Epoch: 10 [1000/1090 (91%)]\tLoss: 0.649225 Accuracy: 896/1090 (82%)\n",
            "\n",
            "Validation set: Average loss: 0.0132, Accuracy: 94/103 (91%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_10.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_10.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 11 [0/1090 (0%)]\tLoss: 0.961422 Accuracy: 14/1090 (1%)\n",
            "Train Epoch: 11 [200/1090 (18%)]\tLoss: 0.420764 Accuracy: 191/1090 (18%)\n",
            "Train Epoch: 11 [400/1090 (36%)]\tLoss: 0.143598 Accuracy: 373/1090 (34%)\n",
            "Train Epoch: 11 [600/1090 (55%)]\tLoss: 0.408137 Accuracy: 553/1090 (51%)\n",
            "Train Epoch: 11 [800/1090 (73%)]\tLoss: 0.335825 Accuracy: 737/1090 (68%)\n",
            "Train Epoch: 11 [1000/1090 (91%)]\tLoss: 0.333025 Accuracy: 913/1090 (84%)\n",
            "\n",
            "Validation set: Average loss: 0.0179, Accuracy: 91/103 (88%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_11.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_11.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 12 [0/1090 (0%)]\tLoss: 0.416526 Accuracy: 18/1090 (2%)\n",
            "Train Epoch: 12 [200/1090 (18%)]\tLoss: 0.172561 Accuracy: 201/1090 (18%)\n",
            "Train Epoch: 12 [400/1090 (36%)]\tLoss: 0.275851 Accuracy: 379/1090 (35%)\n",
            "Train Epoch: 12 [600/1090 (55%)]\tLoss: 0.238607 Accuracy: 561/1090 (51%)\n",
            "Train Epoch: 12 [800/1090 (73%)]\tLoss: 0.389115 Accuracy: 743/1090 (68%)\n",
            "Train Epoch: 12 [1000/1090 (91%)]\tLoss: 0.228383 Accuracy: 926/1090 (85%)\n",
            "\n",
            "Validation set: Average loss: 0.0135, Accuracy: 95/103 (92%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_12.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_12.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 13 [0/1090 (0%)]\tLoss: 0.259870 Accuracy: 19/1090 (2%)\n",
            "Train Epoch: 13 [200/1090 (18%)]\tLoss: 0.267735 Accuracy: 198/1090 (18%)\n",
            "Train Epoch: 13 [400/1090 (36%)]\tLoss: 0.212834 Accuracy: 382/1090 (35%)\n",
            "Train Epoch: 13 [600/1090 (55%)]\tLoss: 0.263905 Accuracy: 563/1090 (52%)\n",
            "Train Epoch: 13 [800/1090 (73%)]\tLoss: 0.405425 Accuracy: 749/1090 (69%)\n",
            "Train Epoch: 13 [1000/1090 (91%)]\tLoss: 0.411802 Accuracy: 930/1090 (85%)\n",
            "\n",
            "Validation set: Average loss: 0.0160, Accuracy: 90/103 (87%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_13.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_13.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 14 [0/1090 (0%)]\tLoss: 0.509689 Accuracy: 17/1090 (2%)\n",
            "Train Epoch: 14 [200/1090 (18%)]\tLoss: 0.331896 Accuracy: 198/1090 (18%)\n",
            "Train Epoch: 14 [400/1090 (36%)]\tLoss: 0.181900 Accuracy: 372/1090 (34%)\n",
            "Train Epoch: 14 [600/1090 (55%)]\tLoss: 0.165778 Accuracy: 548/1090 (50%)\n",
            "Train Epoch: 14 [800/1090 (73%)]\tLoss: 0.536377 Accuracy: 732/1090 (67%)\n",
            "Train Epoch: 14 [1000/1090 (91%)]\tLoss: 0.360995 Accuracy: 911/1090 (84%)\n",
            "\n",
            "Validation set: Average loss: 0.0126, Accuracy: 94/103 (91%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_14.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_14.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 15 [0/1090 (0%)]\tLoss: 0.119371 Accuracy: 19/1090 (2%)\n",
            "Train Epoch: 15 [200/1090 (18%)]\tLoss: 0.409436 Accuracy: 200/1090 (18%)\n",
            "Train Epoch: 15 [400/1090 (36%)]\tLoss: 0.434259 Accuracy: 386/1090 (35%)\n",
            "Train Epoch: 15 [600/1090 (55%)]\tLoss: 0.220729 Accuracy: 572/1090 (52%)\n",
            "Train Epoch: 15 [800/1090 (73%)]\tLoss: 0.372541 Accuracy: 752/1090 (69%)\n",
            "Train Epoch: 15 [1000/1090 (91%)]\tLoss: 0.327007 Accuracy: 934/1090 (86%)\n",
            "\n",
            "Validation set: Average loss: 0.0134, Accuracy: 93/103 (90%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_15.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_15.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 16 [0/1090 (0%)]\tLoss: 0.311769 Accuracy: 18/1090 (2%)\n",
            "Train Epoch: 16 [200/1090 (18%)]\tLoss: 0.134948 Accuracy: 204/1090 (19%)\n",
            "Train Epoch: 16 [400/1090 (36%)]\tLoss: 0.173588 Accuracy: 387/1090 (36%)\n",
            "Train Epoch: 16 [600/1090 (55%)]\tLoss: 0.215422 Accuracy: 567/1090 (52%)\n",
            "Train Epoch: 16 [800/1090 (73%)]\tLoss: 0.143138 Accuracy: 747/1090 (69%)\n",
            "Train Epoch: 16 [1000/1090 (91%)]\tLoss: 0.396707 Accuracy: 923/1090 (85%)\n",
            "\n",
            "Validation set: Average loss: 0.0127, Accuracy: 94/103 (91%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_16.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_16.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 17 [0/1090 (0%)]\tLoss: 0.332218 Accuracy: 19/1090 (2%)\n",
            "Train Epoch: 17 [200/1090 (18%)]\tLoss: 0.324606 Accuracy: 206/1090 (19%)\n",
            "Train Epoch: 17 [400/1090 (36%)]\tLoss: 0.487728 Accuracy: 390/1090 (36%)\n",
            "Train Epoch: 17 [600/1090 (55%)]\tLoss: 0.447180 Accuracy: 573/1090 (53%)\n",
            "Train Epoch: 17 [800/1090 (73%)]\tLoss: 0.247633 Accuracy: 751/1090 (69%)\n",
            "Train Epoch: 17 [1000/1090 (91%)]\tLoss: 0.301244 Accuracy: 940/1090 (86%)\n",
            "\n",
            "Validation set: Average loss: 0.0124, Accuracy: 96/103 (93%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_17.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_17.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 18 [0/1090 (0%)]\tLoss: 0.204056 Accuracy: 20/1090 (2%)\n",
            "Train Epoch: 18 [200/1090 (18%)]\tLoss: 0.175962 Accuracy: 205/1090 (19%)\n",
            "Train Epoch: 18 [400/1090 (36%)]\tLoss: 0.677367 Accuracy: 387/1090 (36%)\n",
            "Train Epoch: 18 [600/1090 (55%)]\tLoss: 0.165112 Accuracy: 565/1090 (52%)\n",
            "Train Epoch: 18 [800/1090 (73%)]\tLoss: 0.043530 Accuracy: 753/1090 (69%)\n",
            "Train Epoch: 18 [1000/1090 (91%)]\tLoss: 0.098300 Accuracy: 939/1090 (86%)\n",
            "\n",
            "Validation set: Average loss: 0.0136, Accuracy: 92/103 (89%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_18.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_18.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 19 [0/1090 (0%)]\tLoss: 0.213644 Accuracy: 18/1090 (2%)\n",
            "Train Epoch: 19 [200/1090 (18%)]\tLoss: 0.258884 Accuracy: 202/1090 (19%)\n",
            "Train Epoch: 19 [400/1090 (36%)]\tLoss: 0.212526 Accuracy: 389/1090 (36%)\n",
            "Train Epoch: 19 [600/1090 (55%)]\tLoss: 0.065185 Accuracy: 577/1090 (53%)\n",
            "Train Epoch: 19 [800/1090 (73%)]\tLoss: 0.352543 Accuracy: 757/1090 (69%)\n",
            "Train Epoch: 19 [1000/1090 (91%)]\tLoss: 0.727789 Accuracy: 938/1090 (86%)\n",
            "\n",
            "Validation set: Average loss: 0.0125, Accuracy: 94/103 (91%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_19.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_19.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 20 [0/1090 (0%)]\tLoss: 0.210621 Accuracy: 19/1090 (2%)\n",
            "Train Epoch: 20 [200/1090 (18%)]\tLoss: 0.484631 Accuracy: 206/1090 (19%)\n",
            "Train Epoch: 20 [400/1090 (36%)]\tLoss: 0.384525 Accuracy: 392/1090 (36%)\n",
            "Train Epoch: 20 [600/1090 (55%)]\tLoss: 0.229012 Accuracy: 584/1090 (54%)\n",
            "Train Epoch: 20 [800/1090 (73%)]\tLoss: 0.142850 Accuracy: 775/1090 (71%)\n",
            "Train Epoch: 20 [1000/1090 (91%)]\tLoss: 0.313203 Accuracy: 956/1090 (88%)\n",
            "\n",
            "Validation set: Average loss: 0.0120, Accuracy: 93/103 (90%)\n",
            "Saved model to /content/gdrive/My Drive/recvis/experiment/model_20.pth. You can run `python evaluate.py --model /content/gdrive/My Drive/recvis/experiment/model_20.pth` to generate the Kaggle formatted csv file\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnlTUu7j9oy6",
        "colab_type": "code",
        "outputId": "38225641-7b25-459c-8213-c09f269aa9b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import PIL.Image as Image\n",
        "\n",
        "import torch\n",
        "\n",
        "parser = argparse.ArgumentParser(description='RecVis A3 evaluation script')\n",
        "parser.add_argument('--data', type=str, default='bird_dataset', metavar='D',\n",
        "                    help=\"folder where data is located. test_images/ need to be found in the folder\")\n",
        "parser.add_argument('--model', type=str, metavar='M',\n",
        "                    help=\"the model file to be evaluated. Usually it is of the form model_X.pth\")\n",
        "parser.add_argument('--outfile', type=str, default='experiment/kaggle.csv', metavar='D',\n",
        "                    help=\"name of the output csv file\")\n",
        "\n",
        "\n",
        "path_read = '/content/drive/My Drive/recvis19_a3-master/bird_dataset'\n",
        "path_write = '/content/drive/My Drive/recvis19_a3-master/experiment'\n",
        "outfile = '/content/drive/My Drive/recvis19_a3-master/experiment/kaggle.csv'\n",
        "seed = 1\n",
        "momentum = 0.5\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.1\n",
        "log_interval = 10\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "state_dict = torch.load('/content/gdrive/My Drive/recvis/experiment/model_12.pth')\n",
        "model = models.resnet101(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs,20)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "if use_cuda:\n",
        "    print('Using GPU')\n",
        "    model.cuda()\n",
        "else:\n",
        "    print('Using CPU')\n",
        "\n",
        "\n",
        "test_dir = path_read + '/test_images/mistery_category'\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB')\n",
        "\n",
        "\n",
        "output_file = open('/content/drive/My Drive/recvis19_a3-master/experiment/kaggle101neww.csv', \"w\")\n",
        "output_file.write(\"Id,Category\\n\")\n",
        "for f in tqdm(os.listdir(test_dir)):\n",
        "    if 'jpg' in f:\n",
        "        data = data_transforms(pil_loader(test_dir + '/' + f))\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n",
        "\n",
        "output_file.close()\n",
        "\n",
        "print(\"Succesfully wrote \" + outfile + ', you can upload this file to the kaggle competition website')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 517/517 [03:55<00:00,  1.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Succesfully wrote /content/drive/My Drive/recvis19_a3-master/experiment/kaggle.csv, you can upload this file to the kaggle competition website\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}